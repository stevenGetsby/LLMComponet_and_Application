{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(21128, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=21128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextGenerationPipeline, GPT2Model\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(21128, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2SdpaAttention(\n",
      "        (c_attn): Conv1D(nf=2304, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=768)\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D(nf=3072, nx=768)\n",
      "        (c_proj): Conv1D(nf=768, nx=3072)\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Model.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GPT2Model' object has no attribute 'lm_head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lm_weight \u001b[38;5;241m=\u001b[39m (\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(lm_weight,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./dataset/lm_weight.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/learning/lib/python3.8/site-packages/torch/nn/modules/module.py:1729\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1728\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2Model' object has no attribute 'lm_head'"
     ]
    }
   ],
   "source": [
    "lm_weight = (model.lm_head.state_dict()[\"weight\"])\n",
    "torch.save(lm_weight,\"./dataset/lm_weight.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        self.model =  GPT2Model.from_pretrained(\"uer/gpt2-chinese-cluecorpussmall\")\n",
    "\n",
    "        self.lm_head = torch.nn.Linear(768,21128,bias=False)\n",
    "        weight = torch.load(\"./dataset/lm_weight.pth\")\n",
    "        self.lm_head.weight = nn.Parameter(weight)\n",
    "\n",
    "        self.value_layer = torch.nn.Sequential(torch.nn.Linear(768,1),torch.nn.Tanh(),torch.nn.Dropout(0.1))\n",
    "\n",
    "    def forward(self,token_inputs):\n",
    "\n",
    "        embedding = self.model(token_inputs)\n",
    "        embedding = embedding[\"last_hidden_state\"]\n",
    "\n",
    "        embedding = torch.nn.Dropout(0.1)(embedding)\n",
    "        logits = self.lm_head(embedding)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1064 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "token_list = []\n",
    "with open(\"./dataset/ChnSentiCorp.txt\", \"r\", encoding=\"UTF-8\") as emotion_file:\n",
    "    for line in emotion_file.readlines():\n",
    "        line = line.strip().split(\",\")\n",
    "        text = \"\".join(line[1:])\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        token = input_ids = inputs[\"input_ids\"]\n",
    "        attention_mask = inputs[\"attention_mask\"]\n",
    "        for id in token[0]:\n",
    "            token_list.append(id.item())\n",
    "token_list = torch.tensor(token_list * 5)\n",
    "\n",
    "class TextSamplerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\t    #下面的写法是为了遵守GPT2数据输入输出格式而特定的写法\n",
    "        rand_start = torch.randint(0, self.data.size(0) - self.seq_len, (1,))\n",
    "        full_seq = self.data[rand_start : rand_start + self.seq_len + 1].long()\n",
    "        return full_seq[:-1],full_seq[1:]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.size(0) // self.seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_192380/170593014.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  weight = torch.load(\"./dataset/lm_weight.pth\")\n",
      "epoch:1, train_loss:1.58591, lr:0.01990: 100%|██████████| 19145/19145 [42:45<00:00,  7.46it/s]  \n",
      "epoch:2, train_loss:1.27436, lr:0.01959: 100%|██████████| 19145/19145 [54:56<00:00,  5.81it/s] \n",
      "epoch:3, train_loss:0.65167, lr:0.01909: 100%|██████████| 19145/19145 [9:57:17<00:00,  1.87s/it]       \n",
      "epoch:4, train_loss:0.72051, lr:0.01840: 100%|██████████| 19145/19145 [49:11<00:00,  6.49it/s] \n",
      "epoch:5, train_loss:1.55126, lr:0.01754: 100%|██████████| 19145/19145 [48:59<00:00,  6.51it/s]\n",
      "epoch:6, train_loss:0.41094, lr:0.01653: 100%|██████████| 19145/19145 [54:13<00:00,  5.88it/s]\n",
      "epoch:7, train_loss:0.53372, lr:0.01538: 100%|██████████| 19145/19145 [57:32<00:00,  5.54it/s] \n",
      "epoch:8, train_loss:0.21933, lr:0.01413: 100%|██████████| 19145/19145 [58:10<00:00,  5.48it/s]\n",
      "epoch:9, train_loss:0.24982, lr:0.01279: 100%|██████████| 19145/19145 [56:49<00:00,  5.61it/s] \n",
      "epoch:10, train_loss:0.20603, lr:0.01139: 100%|██████████| 19145/19145 [59:03<00:00,  5.40it/s]\n",
      "epoch:11, train_loss:0.16911, lr:0.00997: 100%|██████████| 19145/19145 [57:26<00:00,  5.55it/s] \n",
      "epoch:12, train_loss:0.18262, lr:0.00855: 100%|██████████| 19145/19145 [53:03<00:00,  6.01it/s]\n",
      "epoch:13, train_loss:0.23585, lr:0.00716: 100%|██████████| 19145/19145 [52:02<00:00,  6.13it/s]\n",
      "epoch:14, train_loss:0.24482, lr:0.00584: 100%|██████████| 19145/19145 [52:05<00:00,  6.13it/s]\n",
      "epoch:15, train_loss:0.35748, lr:0.00460: 100%|██████████| 19145/19145 [51:13<00:00,  6.23it/s]\n",
      "epoch:16, train_loss:0.23177, lr:0.00348: 100%|██████████| 19145/19145 [51:13<00:00,  6.23it/s]\n",
      "epoch:17, train_loss:0.23592, lr:0.00249: 100%|██████████| 19145/19145 [51:36<00:00,  6.18it/s] \n",
      "epoch:18, train_loss:0.22109, lr:0.00166: 100%|██████████| 19145/19145 [54:02<00:00,  5.90it/s]\n",
      "epoch:19, train_loss:0.22510, lr:0.00100: 100%|██████████| 19145/19145 [56:59<00:00,  5.60it/s] \n",
      "epoch:20, train_loss:0.09168, lr:0.00054: 100%|██████████| 19145/19145 [59:02<00:00,  5.40it/s]\n",
      "epoch:21, train_loss:0.16114, lr:0.00027: 100%|██████████| 19145/19145 [1:01:25<00:00,  5.19it/s]\n",
      "epoch:22, train_loss:0.15773, lr:0.00020: 100%|██████████| 19145/19145 [1:01:27<00:00,  5.19it/s]\n",
      "epoch:23, train_loss:0.13655, lr:0.00034: 100%|██████████| 19145/19145 [1:01:37<00:00,  5.18it/s]\n",
      "epoch:24, train_loss:0.23995, lr:0.00068: 100%|██████████| 19145/19145 [1:01:15<00:00,  5.21it/s]\n",
      "epoch:25, train_loss:0.11184, lr:0.00122: 100%|██████████| 19145/19145 [1:02:52<00:00,  5.07it/s]\n",
      "epoch:26, train_loss:0.22280, lr:0.00194: 100%|██████████| 19145/19145 [1:03:00<00:00,  5.06it/s]\n",
      "epoch:27, train_loss:0.13947, lr:0.00283: 100%|██████████| 19145/19145 [1:04:46<00:00,  4.93it/s]\n",
      "epoch:28, train_loss:0.17766, lr:0.00387: 100%|██████████| 19145/19145 [1:05:00<00:00,  4.91it/s]\n",
      "epoch:29, train_loss:0.21902, lr:0.00504: 100%|██████████| 19145/19145 [1:05:05<00:00,  4.90it/s]\n",
      "epoch:30, train_loss:0.12889, lr:0.00631: 100%|██████████| 19145/19145 [1:03:43<00:00,  5.01it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "max_length = 128 + 1\n",
    "batch_size = 2\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "save_path = \"./train_model_emo.pth\"\n",
    "glm_model = GPT2()\n",
    "glm_model.to(device)\n",
    "#glm_model.load_state_dict(torch.load(save_path),strict=False)\n",
    "optimizer = torch.optim.AdamW(glm_model.parameters(), lr=2e-4)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,T_max = 1200,eta_min=2e-6,last_epoch=-1)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "train_dataset = TextSamplerDataset(token_list,max_length)\n",
    "loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=True)\n",
    "\n",
    "for epoch in range(30):\n",
    "    pbar = tqdm(loader, total=len(loader))\n",
    "    for token_inp,token_tgt in pbar:\n",
    "        token_inp = token_inp.to(device)\n",
    "        token_tgt = token_tgt.to(device)\n",
    "\n",
    "        logits = glm_model(token_inp)\n",
    "        loss = criterion(logits.view(-1,logits.size(-1)),token_tgt.view(-1))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()  # 执行优化器\n",
    "        pbar.set_description(f\"epoch:{epoch +1}, train_loss:{loss.item():.5f}, lr:{lr_scheduler.get_last_lr()[0]*100:.5f}\")\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        torch.save(glm_model.state_dict(),save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
